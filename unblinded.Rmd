---
title: "NBA Team Strength"
author: "Nick Kim, Jia Guo, Suvro Mukherjee"
date: "2025-04-18"
output:
  html_document:
    theme: united
    toc: yes
    toc_float:
      collapsed: true
---

![Rockets SLAM Cover (Slam 254, 2025)](https://drive.google.com/uc?export=view&id=1pIHphgVGkqYD_Y30jos5B0bPQg4v1u3O)

```{r setup, warning=FALSE, message=FALSE}
library(tidyverse)
library(readxl)
library(pomp)
library(doFuture)
library(future.apply)
library(iterators)
library(tsibble)
library(fabletools)
library(feasts)
library(forecast)
```

## Introduction

Basketball, like all sports, is wildly unpredictable. A game filled with runs of hot and cold streaks, any team can win on a given night. However, in all this chaos is there is there an underlying truth? Are some teams just flat out better than another, and can we measure by how much? Luckily there is a traditional method to try and capture such a state, ELO. This simple yet beautiful statistic, introduced by Arpad Elo, sets all teams or players at the same base rating and over time adjusts their score depending on the result of their game and how strong their opponent was (Wikipedia, 2025). However, this simple approach fails to account for the randomness of each event. A team could be having on off night, a crucial player could get injured or traded, all of which would alter their ELO rating on that given day. In this report we'll introduce what we'll call POMP-ELO which will try and remedy these concerns by focusing on modeling the team strength for the Houston Rockets. We'll walk you through our data preparation, our model selection as well as comparison to other baseline models, and finally a conclusion on our results. 

## Data Preparation 

While ELO is a wildly known metric there is no readily available data for tracking a teams ELO and their respective opponents ELO throughout a season. For this reason we had to calculate this ourselves by first pulling all matchups in the 2023-24, and 2024-25 season from the database Basketball Reference. Using the scores and results of each game we then use the formula of ELO to calculate this metric for every team (Wikipedia, 2025):

$$
E_{S} = \frac{1}{1+10^{\frac{TS - OP}{400}}}
$$
$$
TS = TS \pm K\cdot E_{S}
$$
Where $TS$ is team strength and $OP$ opponent strength. Then we updated $TS$ depending on the result of the game, adding if they won and subtracting if they lost. Furthermore the K-factor, how sensitive ELO adjusts to new results, can be adjusted but we opted to use the results found by Australia Sports Tipping in 20 being the optimal value for basketball (Australia Sports Tipping, 2025).

```{r include=FALSE}
matchups <- read_excel("/Users/nicholaskim/Documents/STAT-531/final/data/matchups.xlsx")
```

Code for ELO 
```{r}
update_elo <- function(rating1, rating2, result, K = 20) {
  E1 <- 1 / (1 + 10^((rating2 - rating1) / 400))  # Expected score for team 1
  E2 <- 1 / (1 + 10^((rating1 - rating2) / 400))  # Expected score for team 2
  
  if (result == 1) {  # Team 1 wins
    rating1_new <- rating1 + K * (1 - E1)
    rating2_new <- rating2 + K * (0 - E2)
  } else {  # Team 2 wins
    rating1_new <- rating1 + K * (0 - E1)
    rating2_new <- rating2 + K * (1 - E2)
    
  } 
  
  return(c(rating1_new, rating2_new))
}
```

```{r include=FALSE}
elo_ratings <- data.frame(Team = unique(c(matchups$Visitor, matchups$Home)),
                          Elo = rep(1500, length(unique(c(matchups$Visitor, matchups$Home)))))

matchup23 <- matchups[matchups$Season>=2023,]
matchup23$Result <- ifelse(matchup23$PTS_V > matchup23$PTS_H, 1, 2)  

elo_v <- c()
elo_h <- c()

# Loop through matchups to update Elo ratings
for (i in 1:nrow(matchup23)) {
  team1 <- matchup23$Visitor[i]
  team2 <- matchup23$Home[i]
  
  # Get current ratings using match()
  rating1 <- elo_ratings$Elo[match(team1, elo_ratings$Team)]
  rating2 <- elo_ratings$Elo[match(team2, elo_ratings$Team)]
  
  # Update Elo ratings
  updated_ratings <- update_elo(rating1, rating2, matchup23$Result[i])
  
  # Save updated ratings back into the data frame
  elo_ratings$Elo[elo_ratings$Team == team1] <- updated_ratings[1]
  elo_ratings$Elo[elo_ratings$Team == team2] <- updated_ratings[2]
  
  elo_v <- c(elo_v,updated_ratings[1])
  elo_h <- c(elo_h,updated_ratings[2])
  
}

matchup23$ELO_V <- elo_v
matchup23$ELO_H <- elo_h

r_23 <- matchup23[(matchup23$Visitor=="Houston Rockets")|(matchup23$Home=="Houston Rockets"),]

n <- nrow(r_23)
rockets_elo_df <- data.frame(date = as.Date(rep(NA, n)), elo = rep(NA, n), opp_elo = rep(NA, n))
row_counter <- 1

for (i in 1:n) {
  if (r_23[i, 3] == "Houston Rockets") {
    rockets_elo_df[row_counter, ] <- list(as.Date(r_23[[i, 2]],format = "%a, %b %d, %Y"), as.numeric(r_23[i, 9]), as.numeric(r_23[i,10]))
    row_counter <- row_counter + 1
  } else if (r_23[i, 5] == "Houston Rockets") {
    rockets_elo_df[row_counter, ] <- list(as.Date(r_23[[i, 2]],format = "%a, %b %d, %Y"), as.numeric(r_23[i, 10]),
                                          as.numeric(r_23[i,9]))
    row_counter <- row_counter + 1
  }
}

# Remove unused rows
rockets_elo_df <- rockets_elo_df[1:(row_counter - 1), ]

rockets_elo_df["time"] <- seq(1,164,by=1)
```

The following ELO over time came out to look like
```{r echo=FALSE}
#adding in initial elo at time 0
inital <- data.frame(date = as.Date(10/24/2023,
  format = "%m/%d/%y"),elo=1500,opp_elo=1500,time=0)
rockets_elo_df2 <- rbind(rockets_elo_df,inital)
ggplot(rockets_elo_df2,aes(time,elo))+
  geom_line()+
  labs(title="Rockets ELO over Time")
```

We now have the data for our latent space which we'll try to model using POMP. To do this we'll use the following covariates: Average Last 5 game Total BPM, Home, Opponent ELO, and Average Last 5 Opponent Total BPM (depending on the POMP model we chose). These stats were similarly pulled from Basketball Reference using the Box Score statistics for each game.

Here BPM is a statistics that trys to measure how impactful a certain player was in a given game. For the purposes of our model we added the BPM's for all the starters (5 players that started the game) to get a measure of Total BPM. The rational behind this is that the starters are generally the best players for each team. Thus this measure would give us the best representation of how a team performed on a given night as it'll be drawn from the most impactful players. To take one step further we then took the average of these Total BPM's over their last 5 games to get a measure of team momentum. In the case, the number of games were less than 5 we then took the average with respect to how many games they played up until that point. This is more clearly seen when we take a look at the data: 

```{r include=FALSE}
bpm <- read_excel("/Users/nicholaskim/Documents/STAT-531/final/data/BPM.xls")
bpm["elo"] <- rockets_elo_df$elo
bpm["time"] <- rockets_elo_df$time
bpm["opp_elo"] <- rockets_elo_df$opp_elo
```

```{r echo=FALSE}
bpm[1:10,c('Date','Total BPM','Last 5 Games BPM')]
```

We'll introduce two POMP models where two different approaches were made: 
1. Opp ELO is a covariate 
2. Opp ELO is a state itself
This will become more clear in our POMP section but under the 2nd condition we'll use the Average Last 5 BPM for the opponent to then adjust the Opponent strength, while under the 1st it'll not fluctuate as it's not a random process like team strength. 

## Baseline Models

### Linear Regression 

We'll define a linear regression model to serve as a baseline of our understanding how ELO ratings relate to team performance indicators, independent of any latent process. The model is specified as:

$$
\text{elo}_n = \gamma_0 + \gamma_1 \cdot LVBPM_n + \gamma_2 \cdot OLVBPM_n + \gamma_3 \cdot Home_n + \gamma_4 \cdot Win_n + \varepsilon_n
$$

where:

- $elo_n$ is the observed Elo score of the team before game $n$  
- $LVBPM_n$ is the team's average Total BPM over the last 5 games  
- $OLVBPM_n$ is the opponent's average Total BPM over their last 5 games  
- $Home_n$ is a binary indicator for whether the team played at home  
- $Win_n$ is a binary indicator for whether the team won the game  
- $\varepsilon_n$ is the residual error term  

This model allows us to assess the direct linear contribution of observable variables to Elo scores without recursive updates or stochastic latent states, in contrast to the dynamic modeling used in the POMP framework.

```{r, echo=FALSE, eval=FALSE}
colnames(bpm)
```

```{r}
lm_elo <- lm(elo ~ `Last 5 Games BPM` + `Opp Last5  BPM` + Home + Win, data = bpm)
summary(lm_elo)
```

The baseline linear regression model estimates Elo ratings using observable team performance indicators:

- Team recent performance (`Last 5 Games BPM`) is a strong and statistically significant predictor of Elo, with each point of BPM associated with an average increase of 3.94 Elo points ($p < 0.001$).
- Winning the game also has a significant positive impact, associated with an average increase of 18.84 Elo points ($p = 0.004$).
- Opponent's recent performance (`Opp Last5 BPM`) and home-court status are not statistically significant, indicating limited direct contribution to Elo in this linear framework.
- The model explains 35.3% of the variance in Elo ($R^2 = 0.353$), suggesting that while observable features capture some structure, the remaining variation may be attributed to latent dynamics or factors not included in the model.

```{r echo=FALSE}
plot(lm_elo,sub.caption=" ")
```

1. The residuals vs fitted plot shows no strong non-linearity, but there is noticeable heteroscedasticity and clustering, especially around the center. This suggests unaccounted structure in the residuals, likely from temporal correlation or hidden states that vary across time.

This pattern supports the need for a time-dependent model like POMP or ARIMA, where past performance, dynamics, and latent factors can be explicitly modeled to capture the evolving nature of Elo.

2. The Q-Q plot shows that the residuals are approximately normal, with slight deviations in the tails. This suggests that while the normality assumption mostly holds, there are some mild outliers or unmodeled structure at the extremes.

This supports the earlier observation that a purely linear model may be too rigid, failing to capture dynamic effects or latent processes influencing Elo. It reinforces the motivation for a POMP model or ARIMA, which can better accommodate time-evolving shocks and nonlinear error behavior.

```{r echo=FALSE}
hist(residuals(lm_elo), main = "Residuals Histogram", xlab = "Residuals")

plot(predict(lm_elo), bpm$elo,
     main = "Predicted vs Actual Elo",
     xlab = "Predicted Elo",
     ylab = "Actual Elo")
abline(0, 1, col = "red")
```

The residual diagnostics indicate that the linear model does not fit the Elo data well. The residuals range from approximately $-100$ to $+100$, suggesting substantial unexplained variation. The Residuals vs Fitted plot shows mild heteroscedasticity and scattered structure, while the Q-Q plot reveals slight deviations in the tails, pointing to the presence of outliers or latent dynamics not captured by the model.

These observations suggest that a static linear regression is insufficient for modeling Elo, which is inherently time-dependent and recursively updated. This motivates the use of more appropriate models such as the POMP framework, which treats team strength as a latent process evolving over time, or ARIMA models, which directly account for autocorrelation and temporal structure in observed Elo scores.

### ARIMA

Trying ARIMA fitting. To begin from our time plot of ELO we might be concerned with stationary leading to the choice of a log and 1st difference transformation to our data.
```{r echo=FALSE}
acfs<-acf(diff(log(bpm$elo)),plot=FALSE)
plot(acfs,main="ACF of Transformed ELO")
```

From the plot it seems to indicate that there might be significant correlation in our data among time points with all but one lag being within the confidence interval thus failing to reject our null.

Grid Search for model performance over different AR and MA's
```{r aic-table,  warning=FALSE, message=FALSE}
aic_table <- function(data,P,Q){
table <- matrix(NA,(P+1),(Q+1))
for(p in 0:P) {
for(q in 0:Q) {
table[p+1,q+1] <- arima(data,order=c(p,1,q))$aic
}
}
dimnames(table) <- list(paste("AR",0:P, sep=""),
paste("MA",0:Q,sep=""))
table
}
bpm_aic_table <- aic_table(log(bpm$elo),4,5)
require(knitr)
kable(bpm_aic_table,digits=2)
```
From table we notice that ARMA(2,1,2) seems to be performing the best with AIC of -1188.47!

Just as a sanity check lets see if the units lie within the unit circle showing convertibility.
```{r echo=FALSE, message=FALSE, warning=FALSE}
arima_22 <- Arima(log(bpm$elo),order=c(2,1,2))
plot(arima_22)
```
From the plot above we can see that they do in fact lie within the unit circle for AR but there might be some concerns with our MA as it is on the border of this boundary. 

```{r arma-plot, echo = FALSE, warning=FALSE, message=FALSE}
plot(bpm$time,log(bpm$elo),type="l",xlab="Time",ylab="Log ELO",main="Plot of Fitted ARIMA on Data")
lines(bpm$time,fitted(arima_22),col="blue")
```

```{r arma-norm,echo = FALSE, warning=FALSE, message=FALSE}
acf_res<-acf(arima_22$residuals,plot=FALSE)
plot(acf_res,main="ACF Plot of Residuals")
qqnorm(arima_22$residuals)
qqline(arima_22$residuals, col = "steelblue", lwd = 2)
```
From these other preliminary plots it seems ARIMA might not be the best fit for this data. The QQ plot isn't surprising as the data itself was log transformed which explains the shape of our data. Perhaps there might be a better method?


## POMP 

### Defining Model

As mentioned, two different approaches we're taken, but both models followed the same general POMP structure. That being Team Strength (TS) was first adjusted by how well they performed up until the game $n$ with "LVBPM" being the average of a teams last 5 Total BPM scores. In addition, some noise was added to act as some other underlying variable such as team psychology or level of fatigue during the game which would certainly affect their relative strength. However, if we're not careful, TS can grow to always be bigger than Opponent Strength as it can grow to be larger than the scale of Opponent Strength (1800-1300). Under these cases, the the Team will always be predicted to win as their TS is inflated and not actually representative of how good they are. To combat this we added a regulating term to TS which as adjusted using the parameter $\alpha$

Pre-adjustment
$$
TS_{n} = TS_{n} + \beta_{1}LVBPM_{n} - \alpha(TS_{n}-1500) + \epsilon
$$

Following this pre-adjustment phase, TS was further adjusted using the same ELO logic, adding or subtracting TS by the a metric of how strong their opponent was. 

The prediction for the winner for each matchup was found using the p in the Bradley Terry Model where $hca$ was the parameter for home court advantage (Stanford, 2016). This further level of complexity was added as it's widely known that the home team has a slight advantage in winning a basketball game, thus a boost was given to the home side.  
$$
p = \frac{e^{hca\cdot I(Home=1) + team_1}}{e^{hca\cdot I(Home=1)+team_1}+e^{hca\cdot I(Home=0)+team_2}}
$$

Post-prediction
$$
TS_{n+1} = TS_{n} \pm I(Win=[1,0])(20 \cdot(1 - E))
$$
Where E is:
$$
E = \frac{1}{1 + 10^{\frac{OPP-TS}{ 400}}}
$$

The only difference in both models came in how OPP was represented.

1. OPP as covariate

Here OPP was provided by the data and didn't change for every simulation while...

2. OPP as a state

In this case, we had to represent OPP as a noisy measurement drawn using a similar process as TS. Specifically, we adjusted OPP using Opponent Last 5 AVG BPM (OLVBPM) along with some noise as a separate sate variable and the same regulating factor. 

$$
OPP_{n} = OPP_{n} + \beta_{2}OLVBPM + - \alpha(OPP_{n}-1500) + \epsilon
$$

### Code

Code under 1.

```{r}
rproc <- Csnippet("
  team_strength += beta1 *last5_bpm - alpha * (team_strength - 1500)  + rnorm(0, sigma);
  
  double p_win = 1.0 / (1.0 + pow(10, (opp_strength - team_strength) / 400));
  int sim_win = rbinom(1, p_win); 
  
  if (sim_win == 1) {
    team_strength += 20 * (1 - 1/(1+pow(10, (opp_strength - team_strength)/400)));  
  } else {
    team_strength -= 20 * (1 - 1/(1+pow(10, (opp_strength - team_strength)/400)));  
  }
")

dmeas <- Csnippet("
  double p;
  
  double team_score = team_strength / 100.0;
  double opp_score = opp_strength / 100.0;
  double hca = home_court_avd / 100.0;

  double max_val = fmax(team_score, opp_score);
  
  if (home == 1){
    team_score += hca;
  }
  
  p = exp(team_score - max_val) / (exp(team_score - max_val) + exp(opp_score - max_val));

  lik = dbinom(Win, 1, p, give_log);
")

rmeas <- Csnippet("
double p;
  if (home == 1){
  p = exp(home_court_avd + team_strength - opp_strength) / (1 + exp(home_court_avd + team_strength - opp_strength));
  }
  else{
  p = exp(team_strength - (opp_strength + home_court_avd) ) / (1 + exp(team_strength - (opp_strength + home_court_avd)));
  }
  Win = rbinom(1, p);
")

init <- Csnippet("
  team_strength = 1500;
")

bpm %>% select(time,Win,Home,`Last 5 Games BPM`,opp_elo,elo) -> red_bpm

nba_pomp <- pomp(
  data = red_bpm,
  times = "time",
  t0 = 1,
  rprocess = euler(step.fun = rproc, delta.t = 1),
  rmeasure = rmeas,
  dmeasure = dmeas,
  rinit = init,
  statenames = "team_strength",
  paramnames = c("beta1", "sigma", "home_court_avd","alpha"),
  partrans = parameter_trans(
    log = c("alpha")
  ),
  covar = covariate_table(
    times = red_bpm$time,
    last5_bpm = red_bpm$`Last 5 Games BPM`,
    opp_strength = red_bpm$opp_elo,
    home = red_bpm$Home
  ),
  covarnames = c("last5_bpm","opp_strength","home")
)
```

Change under 2.
```{r}
rproc2 <- Csnippet("
  team_strength += beta1 *last5_bpm  - alpha * (team_strength - 1500) + rnorm(0, sigma);
  opp_strength += beta2 * opp5_bpm - alpha * (opp_strength - 1500) + rnorm(0, sigma);
  
  double p_win = 1.0 / (1.0 + pow(10, (opp_strength - team_strength) / 400));
  int sim_win = rbinom(1, p_win); 
  
  if (sim_win == 1) {
    team_strength += 20 * (1 - 1/(1+pow(10, (opp_strength - team_strength)/400)));  
  } else {
    team_strength -= 20 * (1 - 1/(1+pow(10, (opp_strength - team_strength)/400)));  
  }
")

init2 <- Csnippet("
  team_strength = 1500;
  opp_strength = 1500;
")
```

```{r}
bpm %>% select(time,Win,Home,`Last 5 Games BPM`,`Opp Last5  BPM`,elo) -> red_bpm2

nba_pomp2 <- pomp(
  data = red_bpm2,
  times = "time",
  t0 = 1,
  rprocess = euler(step.fun = rproc2, delta.t = 1),
  rmeasure = rmeas,
  dmeasure = dmeas,
  rinit = init2,
  statenames = c("team_strength","opp_strength"),
  paramnames = c("beta1", "beta2","sigma", "home_court_avd","alpha"),
  covar = covariate_table(
    times = red_bpm2$time,
    last5_bpm = red_bpm2$`Last 5 Games BPM`,
    opp5_bpm = red_bpm2$`Opp Last5  BPM`,
    home = red_bpm2$Home
  ),
  covarnames = c("last5_bpm","opp5_bpm","home")
)
```

### Simulations 

The following are simulations for both models under some set parameter values.

Under Model 1


```{r echo=FALSE, warning=FALSE}
nba_pomp |>
  simulate(
    params=c(beta1=.5,sigma=1,home_court_avd=40,alpha=.05),
    nsim=20,format="data.frame",include.data=TRUE
  ) -> sims

sims |>
  ggplot(aes(x=time,y=team_strength,group=.id,color=.id=="data"))+
  geom_line()+
  geom_line(aes(x=time,y=elo))+
  guides(color="none")+
  labs(title="Simulation Over Fixed Parameters For Model 1")
```

Model 2



```{r echo=FALSE, warning=FALSE}
nba_pomp2 |>
  simulate(
    params=c(beta1=.5,beta2=.5,sigma=1,home_court_avd=40,alpha=.05),
    nsim=20,format="data.frame",include.data=TRUE
  ) -> sims2

sims2 |>
  ggplot(aes(x=time,y=team_strength,group=.id,color=.id=="data"))+
  geom_line()+
  geom_line(aes(x=time,y=elo))+
  guides(color="none")+
  labs(title="Simulation Over Fixed Parameters For Model 2")
```


We can see that both models seem to capture the trend of the ELO with Model 1 having less variance. This is to be expected as in Model 2 we've introduced another level of randomness.

### Local Search 

Model 1

```{r echo=FALSE}
coef(nba_pomp) <- c(beta1=0.5, sigma=1, home_court_avd=40, alpha=0.05)
fixed_params <- coef(nba_pomp,c("sigma"))

plan(multisession)
ncpu <- nbrOfWorkers()
local_mifs <- future_lapply(future.seed=TRUE,seq_len(ncpu), function(i) {
  nba_pomp |>
    mif2(
      Np=1000, Nmif=20,
      cooling.fraction.50=0.5,
      rw.sd=rw_sd(beta1=0.5, home_court_avd=40, alpha=0.05)
    )
})

local_mifs_combined <- do.call(c, local_mifs)

local_mifs_combined |>
  traces(pars=c("loglik","beta1","sigma","home_court_avd","alpha")) |>
  melt() |>
  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+
  geom_line()+
  guides(color="none")+
  facet_wrap(~name,scales="free_y")
```

Model 2


```{r echo=FALSE}
coef(nba_pomp2) <- c(beta1=0.5, beta2=.5,sigma=1, home_court_avd=40,alpha=.05)
fixed_params <- coef(nba_pomp2,c("sigma"))

plan(multisession)
ncpu <- nbrOfWorkers()
local_mifs2 <- future_lapply(future.seed=TRUE,seq_len(ncpu), function(i) {
  nba_pomp2 |>
    mif2(
      Np=1000, Nmif=20,
      cooling.fraction.50=0.5,
      rw.sd=rw_sd(beta1=0.5, home_court_avd=40, beta2=0.5,alpha=.05)
    )
})

local_mifs_combined2 <- do.call(c, local_mifs2)

local_mifs_combined2 |>
  traces(pars=c("loglik","beta1","beta2","sigma","home_court_avd","alpha")) |>
  melt() |>
  ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+
  geom_line()+
  guides(color="none")+
  facet_wrap(~name,scales="free_y")
```

From just these local searches we can see that Model 2 have more noisy estimates of the parameters which makes sense due to the extra randomness in its process. However, it's interesting to note that the alpha parameter still converges is a smooth manner like in Model 1.

### Global Search

Model 1

```{r echo=FALSE}
set.seed(2062379496)
runif_design(
lower=c(beta1=0,home_court_avd=200,alpha=0),
upper=c(beta1=1,home_court_avd=250,alpha=1),
nseq=400
) -> guesses

mf1 <- local_mifs[[1]]

foreach(guess=iter(guesses,"row"), .combine=rbind,
.options.future=list(seed=1270401374)
) %dofuture% {
  mf1 |>
    mif2(params=c(guess,fixed_params)) |>
    mif2(Nmif=20) -> mf
  replicate(
  10,
  mf |> pfilter(Np=1000) |> logLik()
  ) |>
    logmeanexp(se=TRUE) -> ll
  mf |> coef() |> bind_rows() |>
    bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results

results[results$loglik == max(results$loglik),]
```

Model 2


```{r echo=FALSE}
set.seed(2062379496)
runif_design(
lower=c(beta1=0,home_court_avd=200,beta2=0,alpha=0),
upper=c(beta1=1,home_court_avd=250,beta2=1,alpha=1),
nseq=400
) -> guesses

mf1 <- local_mifs2[[1]]

foreach(guess=iter(guesses,"row"), .combine=rbind,
.options.future=list(seed=1270401374)
) %dofuture% {
  mf1 |>
    mif2(params=c(guess,fixed_params)) |>
    mif2(Nmif=20) -> mf
  replicate(
  10,
  mf |> pfilter(Np=1000) |> logLik()
  ) |>
    logmeanexp(se=TRUE) -> ll
  mf |> coef() |> bind_rows() |>
    bind_cols(loglik=ll[1],loglik.se=ll[2])
} -> results2

results2[results2$loglik == max(results2$loglik),]
```

### Attendence Effect

As Students at UMich we know first hand that not all home fields are the same as how it's currently assumped in our POMP model. To account for this we'll try introducing Attendance metrics as a measure of the impact of a teams Home Field Advantage.

```{r include=FALSE}
bpm_att <- read_excel("/Users/nicholaskim/Documents/STAT-531/final/data/BPM-new.xlsx")

bpm_att["elo"] <- rockets_elo_df$elo
bpm_att["time"] <- rockets_elo_df$time
bpm_att["opp_elo"] <- rockets_elo_df$opp_elo
```

Let us visualize how the Home and Attendance are related to each other and give a sense of team support
```{r echo=FALSE}
ggplot(bpm_att, aes(x = factor(Home, labels = c("Away", "Home")), y = Attendance)) +
    geom_boxplot(fill = "skyblue") +
    labs(x = "Game Location", y = "Attendance", title = "Attendance by Game Location")
```

Similarly, let us observe how the win can be affected using an interaction between the home and attendance through both ARMA and a linear regression model

```{r}
model <- glm(Win ~ Home * log(Attendance), data = bpm_att, family = "binomial" )
summary(model)
```

We add an interaction term as an exogenous variable to the arma model autoregressing on the elo
```{r warning=FALSE}
bpm_att$home_attendance <- bpm_att$Home * bpm_att$Attendance
arima_22_att <- arima(log(bpm_att$elo),order=c(2,1,2), xreg = c(bpm_att$home_attendance))
arima_22_att
```

Now let us finally update the rprocess and the pomp model accordingly. Building off of Model 1 as its a simpler model. 
```{r}
rproc_att <- Csnippet("
  team_strength += beta1 *last5_bpm + beta2 * attendance_int - alpha * (team_strength - 1500)  + rnorm(0, sigma);
  
  double p_win = 1.0 / (1.0 + pow(10, (opp_strength - team_strength) / 400));
  int sim_win = rbinom(1, p_win); 
  
  if (sim_win == 1) {
    team_strength += 20 * (1 - 1/(1+pow(10, (opp_strength - team_strength)/400)));  
  } else {
    team_strength -= 20 * (1 - 1/(1+pow(10, (opp_strength - team_strength)/400)));  
  }
")

bpm_att %>% select(time,Win,Home,`Last 5 Games BPM`,opp_elo,elo, home_attendance) -> red_bpm_att

nba_pomp_att <- pomp(
    data = red_bpm_att,
    times = "time",
    t0 = 1,
    rprocess = euler(step.fun = rproc_att, delta.t = 1),
    rmeasure = rmeas,
    dmeasure = dmeas,
    rinit = init,
    statenames = "team_strength",
    paramnames = c("beta1", "beta2", "sigma", "home_court_avd","alpha"),
    partrans = parameter_trans(
        log = c("alpha")
    ),
    covar = covariate_table(
        times = red_bpm_att$time,
        last5_bpm = red_bpm_att$`Last 5 Games BPM`,
        opp_strength = red_bpm_att$opp_elo,
        home = red_bpm_att$Home,
        attendance_int = red_bpm_att$home_attendance
    ),
    covarnames = c("last5_bpm","opp_strength","home", "attendance_int")
)
```

Simulation
```{r warning=FALSE}
nba_pomp_att |>
    simulate(
        params=c(beta1=.5, beta2 = 0.0001, sigma=1,home_court_avd=40,alpha=.05),
        nsim=20,format="data.frame",include.data=TRUE
    ) -> sims

sims |>
    ggplot(aes(x=time,y=team_strength,group=.id,color=.id=="data"))+
    geom_line()+
    geom_line(aes(x=time,y=elo))+
    guides(color="none")+
    labs(title="Simulation Over Fixed Parameters Attendence POMP")
```



Conducting Local Search:
```{r}
coef(nba_pomp_att) <- c(beta1=0.5, beta2 =0.0001, sigma=1, home_court_avd=40, alpha=0.05)
fixed_params <- coef(nba_pomp_att,c("sigma"))

plan(multisession)
ncpu <- nbrOfWorkers()
local_mifs <- future_lapply(future.seed=TRUE,seq_len(ncpu), function(i) {
    nba_pomp_att |>
        mif2(
            Np=1000, Nmif=20,
            cooling.fraction.50=0.5,
            rw.sd=rw_sd(beta1=0.5, beta2=0.0001, home_court_avd=40, alpha=0.05)
        )
})

local_mifs_combined <- do.call(c, local_mifs)

local_mifs_combined |>
    traces(pars=c("loglik","beta1","beta2","sigma","home_court_avd","alpha")) |>
    melt() |>
    ggplot(aes(x=iteration,y=value,group=.L1,color=factor(.L1)))+
    geom_line()+
    guides(color="none")+
    facet_wrap(~name,scales="free_y")

```

## Model Comparison

Comparing Log Likelihoods for each model

Our Baseline models:
```{r echo=FALSE}
sigma <- summary(lm_elo)$sigma
sigma_lm <- sigma*sqrt((164-dim(model.matrix(lm_elo))[2])/164) 
cat("Loglik for Linear Regression:",sum(log(dnorm(x = bpm$elo, mean = predict(lm_elo), sd = sigma_lm))),"\n")
cat("Loglik for ARIMA(2,1,2):",arima_22$loglik)
```

Our POMP Models
```{r echo=FALSE}
cat("Loglik for POMP Model 1:", mean(results$loglik),"\n")
cat("Loglik for POMP Model 2:", mean(results2$loglik))
```

Just from the looks of the log likelihood's it seems ARIMA is a better performing model. However, we should be cautious as noted in the ARMIA section its poly roots for MA are on the unit circle meaning this estimate may be unstable.

A positive in our POMP-ELO approach is that is offers a more granular approach in the effects that play into winning a basketball game which may be more realistic than the traditional ELO statistic. In addition, this approach seems to provide a better model fit in comparison to the simple linear regression approach which is a positive.

## Conclusion 

In such a competitive and results driven world we are often enamored with the concept of ability. How am I doing compared to my peers and where do I rank? How good am I at performing my job; at my craft? Basketball is no exception. With the introduction of ELO we are able to grasp at this measure of team strength in all competitive sports. However, the world is random, its chaotic, two assumptions ELO fails to recognize. While POMP-ELO is by no measure a perfect solution, we feel from this report we've made a case that it offers an interesting yet still intuitive and effective result in filling in the gaps were ELO falls short. 

There are certainty gaps in our report and modeling of ELO. One in that we are using simulated p to predicted the winner of each event. This might violate our latent state assumption as now there is a deterministic process using a process that should be unobservable. This might make the inference team_strength as strictly a latent state invalid but we feel it still offers an interesting perspective in terms of simulation. For this reason an interesting extension could be seeing the models performance in forecasting ELO for future events.

## References

- “Basketball Statistics & History of Every Team & NBA and WNBA Players.” Basketball, www.basketball-reference.com/. Accessed 18 Apr. 2025. 
- “Elo Rating System.” Wikipedia, Wikimedia Foundation, 29 Mar. 2025, en.wikipedia.org/wiki/Elo_rating_system. 
“NBA - Elo Ratings.” Sports, www.aussportstipping.com/sports/nba/elo_ratings/#:~:text=If%20k%20is%20set%20too,the%20optimal%20k%20is%2020.&text=The%20same%20website%20also%20applied,their%20NFL%20Elo%20rating%20system. Accessed 18 Apr. 2025. 
- Staff, SLAM. “Houston Rockets Cover Slam 254.” SLAM, 6 Feb. 2025, www.slamonline.com/the-magazine/slam-254/houston-rockets-254/. 
- Ionides, Edward L. “Lesson 4.
Iterated filtering: principles and practice” Iterated filtering Lecture. Iterated filtering Models Lecture, 22 Mar 2025, Ann Arbor, Michigan .
- “The Bradley-Terry Model.” Lecture24. Lecture 24, autumn 2016, Stanford, California, https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture24.pdf. Accessed 12 Apr. 2025. 
- “Chatgpt.” ChatGPT, chatgpt.com/. Accessed 12 April. 2025.
- 2022, Midterm Project, https://ionides.github.io/531w22/final_project/project03/blinded.pdf. Accessed 12 April. 2025. 

